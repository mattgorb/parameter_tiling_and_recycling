# Architecture
arch: Conv6

# ===== Dataset ===== #
data: /s/luffy/b/nobackup/mgorb/data
set: CIFAR10
name: baseline

# ===== Learning Rate Policy ======== #
optimizer: sgd
lr: 0.1
lr_policy: cosine_lr

# ===== Network training config ===== #
epochs: 500
weight_decay: 0.0001
momentum: 0.9
batch_size: 128

# ===== Sparsity =========== #
layer_type: SubnetConvTiledFull

weight_tile_size: 32768
#global_compression_factor: 4
#layer_compression_factors: 2,4,4,4,4,4,2,2,2
#layer_compression_factors: 1,1,1,2,2,4,8,1,1

#layer_compression_factors: 1,1,1,1,1,1,1,1,1

#layer_compression_factors: 4,4,4,4,4,4,4,4,4

layer_compression_factors: 1,1,1,8,8,8,8,1,1
#layer_compression_factors: 8,8,8,8,8,8,8,8,8

#weight_tile_size: 32768
data_type: float16

bn_type: NonAffineBatchNorm
freeze_weights: True

weight_init: kaiming_normal
scale_fan: True
score_init: None
weight_seed: 0
score_seed: 0


model_type: binarize
alpha_type: multiple


gpu: 0
# ===== Hardware setup ===== #
workers: 4
