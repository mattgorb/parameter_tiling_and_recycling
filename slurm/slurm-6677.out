=> Reading YAML config from ../configs/imagenet/resnet18-tiled-full-rerandtile.yaml
Namespace(data='/s/babbage/b/nobackup/nblancha/public-datasets/subnetworks/', optimizer='sgd', set='ImageNet', arch='ResNet18', config='../configs/imagenet/resnet18-tiled-full-rerandtile.yaml', log_dir=None, workers=20, epochs=100, total_epochs=1000, start_epoch=None, batch_size=128, lr=0.256, warmup_length=5, momentum=0.875, weight_decay=3.0517578125e-05, print_freq=50, num_classes=10, resume='', evaluate=False, pretrained=None, pretrained2=None, local_rank=0, rank=0, world_size=1, ngpus_per_node=6, local_world_size=1, gpu=0, lr_policy='cosine_lr', multistep_lr_adjust=30, multistep_lr_gamma=0.1, name='biprop', save_every=-1, prune_rate=-1, low_data=1, width_mult=1.0, nesterov=False, random_subnet=False, one_batch=False, conv_type='SubnetConvTiledFull', freeze_weights=True, mode='fan_in', nonlinearity='relu', bn_type='NonAffineBatchNorm', no_bn_decay=False, scale_fan=True, first_layer_dense=False, last_layer_dense=False, label_smoothing=0.1, first_layer_type=None, trainer='default', threshold=None, weight_init='kaiming_normal', score_init=None, rerand_iter_freq=None, rerand_epoch_freq=20, rerand_type='rerandomize_and_tile', rerand_warmup=1, rerand_rate=None, weight_seed=0, score_seed=0, seed=0, ablation=False, multigpu='0,1,2', global_mask_compression_factor=4, weight_tile_size=65536, data_type='float16', layer_mask_compression_factors=None, model_type='binarize', alpha_type='multiple')
gpus: ['0', '1', '2'], number: 3
=> Reading YAML config from ../configs/imagenet/resnet18-tiled-full-rerandtile.yaml
=> Using trainer from trainers.default
 GPU 1
Use GPU: 1 for training
=> Creating model 'ResNet18'
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
weight tile: tensor([ 1, -1,  1,  ...,  1,  1,  1])
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
==> Building first layer with <class 'utils.conv_type.SubnetConvTiledFull'>
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
ResNet(
  (conv1): SubnetConvTiledFull(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): SubnetConvTiledFull(512, 1000, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
=> Dense model params:
	11,678,912
=> Tiled params: 
	 2,985,264
name: conv1.weight, i: 0, weight size: 9408, compress factor: 4
name: layer1.0.conv1.weight, i: 1, weight size: 36864, compress factor: 4
name: layer1.0.conv2.weight, i: 2, weight size: 36864, compress factor: 4
name: layer1.1.conv1.weight, i: 3, weight size: 36864, compress factor: 4
name: layer1.1.conv2.weight, i: 4, weight size: 36864, compress factor: 4
name: layer2.0.conv1.weight, i: 5, weight size: 73728, compress factor: 4
name: layer2.0.conv2.weight, i: 6, weight size: 147456, compress factor: 4
name: layer2.0.downsample.0.weight, i: 7, weight size: 8192, compress factor: 4
name: layer2.1.conv1.weight, i: 8, weight size: 147456, compress factor: 4
name: layer2.1.conv2.weight, i: 9, weight size: 147456, compress factor: 4
name: layer3.0.conv1.weight, i: 10, weight size: 294912, compress factor: 4
name: layer3.0.conv2.weight, i: 11, weight size: 589824, compress factor: 4
name: layer3.0.downsample.0.weight, i: 12, weight size: 32768, compress factor: 4
name: layer3.1.conv1.weight, i: 13, weight size: 589824, compress factor: 4
name: layer3.1.conv2.weight, i: 14, weight size: 589824, compress factor: 4
name: layer4.0.conv1.weight, i: 15, weight size: 1179648, compress factor: 4
name: layer4.0.conv2.weight, i: 16, weight size: 2359296, compress factor: 4
name: layer4.0.downsample.0.weight, i: 17, weight size: 131072, compress factor: 4
name: layer4.1.conv1.weight, i: 18, weight size: 2359296, compress factor: 4
name: layer4.1.conv2.weight, i: 19, weight size: 2359296, compress factor: 4
name: fc.weight, i: 20, weight size: 512000, compress factor: 4
=> Freezing model weights
==> No gradient to conv1.weight
==> No gradient to layer1.0.conv1.weight
==> No gradient to layer1.0.conv2.weight
==> No gradient to layer1.1.conv1.weight
==> No gradient to layer1.1.conv2.weight
==> No gradient to layer2.0.conv1.weight
==> No gradient to layer2.0.conv2.weight
==> No gradient to layer2.0.downsample.0.weight
==> No gradient to layer2.1.conv1.weight
==> No gradient to layer2.1.conv2.weight
==> No gradient to layer3.0.conv1.weight
==> No gradient to layer3.0.conv2.weight
==> No gradient to layer3.0.downsample.0.weight
==> No gradient to layer3.1.conv1.weight
==> No gradient to layer3.1.conv2.weight
==> No gradient to layer4.0.conv1.weight
==> No gradient to layer4.0.conv2.weight
==> No gradient to layer4.0.downsample.0.weight
==> No gradient to layer4.1.conv1.weight
==> No gradient to layer4.1.conv2.weight
==> No gradient to fc.weight
set distributed data parallel
rank: 1
gpu: 1
world size: 3
=> Getting ImageNet dataset
=> Reading YAML config from ../configs/imagenet/resnet18-tiled-full-rerandtile.yaml
=> Using trainer from trainers.default
 GPU 0
Use GPU: 0 for training
=> Creating model 'ResNet18'
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
weight tile: tensor([ 1,  1,  1,  ...,  1, -1,  1])
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
==> Building first layer with <class 'utils.conv_type.SubnetConvTiledFull'>
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
ResNet(
  (conv1): SubnetConvTiledFull(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): SubnetConvTiledFull(512, 1000, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
=> Dense model params:
	11,678,912
=> Tiled params: 
	 2,985,264
name: conv1.weight, i: 0, weight size: 9408, compress factor: 4
name: layer1.0.conv1.weight, i: 1, weight size: 36864, compress factor: 4
name: layer1.0.conv2.weight, i: 2, weight size: 36864, compress factor: 4
name: layer1.1.conv1.weight, i: 3, weight size: 36864, compress factor: 4
name: layer1.1.conv2.weight, i: 4, weight size: 36864, compress factor: 4
name: layer2.0.conv1.weight, i: 5, weight size: 73728, compress factor: 4
name: layer2.0.conv2.weight, i: 6, weight size: 147456, compress factor: 4
name: layer2.0.downsample.0.weight, i: 7, weight size: 8192, compress factor: 4
name: layer2.1.conv1.weight, i: 8, weight size: 147456, compress factor: 4
name: layer2.1.conv2.weight, i: 9, weight size: 147456, compress factor: 4
name: layer3.0.conv1.weight, i: 10, weight size: 294912, compress factor: 4
name: layer3.0.conv2.weight, i: 11, weight size: 589824, compress factor: 4
name: layer3.0.downsample.0.weight, i: 12, weight size: 32768, compress factor: 4
name: layer3.1.conv1.weight, i: 13, weight size: 589824, compress factor: 4
name: layer3.1.conv2.weight, i: 14, weight size: 589824, compress factor: 4
name: layer4.0.conv1.weight, i: 15, weight size: 1179648, compress factor: 4
name: layer4.0.conv2.weight, i: 16, weight size: 2359296, compress factor: 4
name: layer4.0.downsample.0.weight, i: 17, weight size: 131072, compress factor: 4
name: layer4.1.conv1.weight, i: 18, weight size: 2359296, compress factor: 4
name: layer4.1.conv2.weight, i: 19, weight size: 2359296, compress factor: 4
name: fc.weight, i: 20, weight size: 512000, compress factor: 4
=> Freezing model weights
==> No gradient to conv1.weight
==> No gradient to layer1.0.conv1.weight
==> No gradient to layer1.0.conv2.weight
==> No gradient to layer1.1.conv1.weight
==> No gradient to layer1.1.conv2.weight
==> No gradient to layer2.0.conv1.weight
==> No gradient to layer2.0.conv2.weight
==> No gradient to layer2.0.downsample.0.weight
==> No gradient to layer2.1.conv1.weight
==> No gradient to layer2.1.conv2.weight
==> No gradient to layer3.0.conv1.weight
==> No gradient to layer3.0.conv2.weight
==> No gradient to layer3.0.downsample.0.weight
==> No gradient to layer3.1.conv1.weight
==> No gradient to layer3.1.conv2.weight
==> No gradient to layer4.0.conv1.weight
==> No gradient to layer4.0.conv2.weight
==> No gradient to layer4.0.downsample.0.weight
==> No gradient to layer4.1.conv1.weight
==> No gradient to layer4.1.conv2.weight
==> No gradient to fc.weight
set distributed data parallel
rank: 0
gpu: 0
world size: 3
=> Getting ImageNet dataset
=> Reading YAML config from ../configs/imagenet/resnet18-tiled-full-rerandtile.yaml
=> Using trainer from trainers.default
 GPU 2
Use GPU: 2 for training
=> Creating model 'ResNet18'
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
weight tile: tensor([ 1, -1,  1,  ..., -1, -1, -1])
==> Conv Type: SubnetConvTiledFull
==> BN Type: NonAffineBatchNorm
==> Building first layer with <class 'utils.conv_type.SubnetConvTiledFull'>
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
weight initialization: kaiming_normal
Using default score initialization
weight_tiled weights with size 65536
ResNet(
  (conv1): SubnetConvTiledFull(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): SubnetConvTiledFull(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (downsample): Sequential(
        (0): SubnetConvTiledFull(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): SubnetConvTiledFull(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): NonAffineBatchNorm(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc): SubnetConvTiledFull(512, 1000, kernel_size=(1, 1), stride=(1, 1), bias=False)
)
=> Dense model params:
	11,678,912
=> Tiled params: 
	 2,985,264
name: conv1.weight, i: 0, weight size: 9408, compress factor: 4
name: layer1.0.conv1.weight, i: 1, weight size: 36864, compress factor: 4
name: layer1.0.conv2.weight, i: 2, weight size: 36864, compress factor: 4
name: layer1.1.conv1.weight, i: 3, weight size: 36864, compress factor: 4
name: layer1.1.conv2.weight, i: 4, weight size: 36864, compress factor: 4
name: layer2.0.conv1.weight, i: 5, weight size: 73728, compress factor: 4
name: layer2.0.conv2.weight, i: 6, weight size: 147456, compress factor: 4
name: layer2.0.downsample.0.weight, i: 7, weight size: 8192, compress factor: 4
name: layer2.1.conv1.weight, i: 8, weight size: 147456, compress factor: 4
name: layer2.1.conv2.weight, i: 9, weight size: 147456, compress factor: 4
name: layer3.0.conv1.weight, i: 10, weight size: 294912, compress factor: 4
name: layer3.0.conv2.weight, i: 11, weight size: 589824, compress factor: 4
name: layer3.0.downsample.0.weight, i: 12, weight size: 32768, compress factor: 4
name: layer3.1.conv1.weight, i: 13, weight size: 589824, compress factor: 4
name: layer3.1.conv2.weight, i: 14, weight size: 589824, compress factor: 4
name: layer4.0.conv1.weight, i: 15, weight size: 1179648, compress factor: 4
name: layer4.0.conv2.weight, i: 16, weight size: 2359296, compress factor: 4
name: layer4.0.downsample.0.weight, i: 17, weight size: 131072, compress factor: 4
name: layer4.1.conv1.weight, i: 18, weight size: 2359296, compress factor: 4
name: layer4.1.conv2.weight, i: 19, weight size: 2359296, compress factor: 4
name: fc.weight, i: 20, weight size: 512000, compress factor: 4
=> Freezing model weights
==> No gradient to conv1.weight
==> No gradient to layer1.0.conv1.weight
==> No gradient to layer1.0.conv2.weight
==> No gradient to layer1.1.conv1.weight
==> No gradient to layer1.1.conv2.weight
==> No gradient to layer2.0.conv1.weight
==> No gradient to layer2.0.conv2.weight
==> No gradient to layer2.0.downsample.0.weight
==> No gradient to layer2.1.conv1.weight
==> No gradient to layer2.1.conv2.weight
==> No gradient to layer3.0.conv1.weight
==> No gradient to layer3.0.conv2.weight
==> No gradient to layer3.0.downsample.0.weight
==> No gradient to layer3.1.conv1.weight
==> No gradient to layer3.1.conv2.weight
==> No gradient to layer4.0.conv1.weight
==> No gradient to layer4.0.conv2.weight
==> No gradient to layer4.0.downsample.0.weight
==> No gradient to layer4.1.conv1.weight
==> No gradient to layer4.1.conv2.weight
==> No gradient to fc.weight
set distributed data parallel
rank: 2
gpu: 2
world size: 3
=> Getting ImageNet dataset
Traceback (most recent call last):
  File "/s/chopin/l/grad/mgorb/parameter_tiling_and_recycling/slurm/../main_parallel_kestrel.py", line 597, in <module>
    mp.spawn(main_worker, nprocs=args.world_size, args=( args,))
  File "/usr/local/python-env/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/python-env/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/usr/local/python-env/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/python-env/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/s/chopin/l/grad/mgorb/parameter_tiling_and_recycling/main_parallel_kestrel.py", line 109, in main_worker
    data = get_dataset(args)
  File "/s/chopin/l/grad/mgorb/parameter_tiling_and_recycling/main_parallel_kestrel.py", line 389, in get_dataset
    dataset = getattr(data, args.set)(args)
  File "/s/chopin/l/grad/mgorb/parameter_tiling_and_recycling/data/imagenet.py", line 83, in __init__
    self.train_loader = torch.utils.data.DataLoader(
  File "/usr/local/python-env/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 324, in __init__
    raise ValueError('sampler option is mutually exclusive with '
ValueError: sampler option is mutually exclusive with shuffle

